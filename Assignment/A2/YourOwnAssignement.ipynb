{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Required Libraries"
      ],
      "metadata": {
        "id": "Uw1Pa7gTuyg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bDh1oQAzqktm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting zip"
      ],
      "metadata": {
        "id": "_74lDTH9u2IA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WkCx9qUhL7s"
      },
      "outputs": [],
      "source": [
        "\n",
        "zip_file_path = '/content/Khaleej-2004-utf8.zip'\n",
        "extract_folder = 'extracted_corpus'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting folder of International news"
      ],
      "metadata": {
        "id": "lREczgA1vAFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "international_folder = os.path.join(extract_folder, '/content/extracted_corpus/Khaleej-2004/International news')"
      ],
      "metadata": {
        "id": "WNNa6W_AqLUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building corpus contituting of all html files that are in the international news folder"
      ],
      "metadata": {
        "id": "yLGnNlfsvDaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for file_path in glob.glob(os.path.join(international_folder, '*.html')):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        html_content = file.read()\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text_content = soup.get_text()\n",
        "        # Add any additional preprocessing steps here\n",
        "        corpus.append(text_content)\n"
      ],
      "metadata": {
        "id": "O-sAbYwvqe1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading nltk libraries"
      ],
      "metadata": {
        "id": "PDsvQszMvMPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Arabic stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('arabic'))\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVggTBmMrkq3",
        "outputId": "3b84877c-610c-40ce-8a29-9792a9f6f23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70wnvVsPsiIg",
        "outputId": "d3ccaa91-d482-4999-ff93-0171399a6983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "dwjG3M4Qsyf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "1t432zrzspZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the corpus"
      ],
      "metadata": {
        "id": "XopNywGvvQD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus = []\n",
        "\n",
        "for document in corpus:\n",
        "    tokens = word_tokenize(document)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    preprocessed_corpus.append(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "Z53wzbaxrskJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-Speech Tagging\n",
        "pos_tags = []\n",
        "for document_tokens in preprocessed_corpus:\n",
        "    pos_tags.extend(pos_tag(document_tokens))"
      ],
      "metadata": {
        "id": "5p0ZjI0DtKWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_tags[0:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LLLQz0ivoCJ",
        "outputId": "c3cbf250-7d6e-440a-9eb3-8e8752fa34bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('سوريا', 'JJ'), ('توفد', 'NNP'), ('المعلم', 'NNP'), ('بيروت', 'NNP'), ('لإجراء', 'NNP'), ('محادثات', 'NNP'), ('موالين', 'NNP'), ('ومعارضين', 'NNP'), ('لتواجدها', 'NNP'), ('بلبنان', 'NNP'), ('انتهجت', 'NNP'), ('سوريا', 'NNP'), ('أسلوبا', 'NNP'), ('جديدا', 'NNP'), ('التعامل', 'NNP'), ('لبنان', 'NNP'), ('بإرسالها', 'NNP'), ('الثلاثاء', 'NNP'), ('مسئولا', 'NNP'), ('رسميا', 'NNP'), ('فتح', 'NNP'), ('قنوات', 'NNP'), ('الحوار', 'NNP'), ('مسئولين', 'NNP'), ('سياسيين', 'NNP'), ('موالين', 'NNP'), ('ومعارضين', 'NNP'), ('لوجودها', 'NNP'), ('لبنان', 'NNP'), ('ووصل', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Frequency Graph\n",
        "all_tokens = [token for doc in preprocessed_corpus for token in doc]\n",
        "word_freq = Counter(all_tokens)"
      ],
      "metadata": {
        "id": "yYeT9hRJtQX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing 10 most frequent words + frequency of each part of speach tag"
      ],
      "metadata": {
        "id": "EhSoOLrJvYHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Top 10 Most Frequent Words\n",
        "print(\"Top 10 Most Frequent Words:\")\n",
        "for word, freq in word_freq.most_common(10):\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "# Count of Each Part-of-Speech Tag\n",
        "pos_tag_counts = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "# Print Number of Each Part-of-Speech Tag\n",
        "print(\"\\nNumber of Each Part-of-Speech Tag:\")\n",
        "for tag, count in pos_tag_counts.items():\n",
        "    print(f\"{tag}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tsy4a3LtXQv",
        "outputId": "163429d6-60c7-4d15-a196-145928404725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Words:\n",
            "ان: 9518\n",
            "الى: 5274\n",
            "وقال: 3158\n",
            "امس: 1911\n",
            "العراق: 1777\n",
            "الانتخابات: 1610\n",
            "المتحدة: 1401\n",
            "انه: 1351\n",
            "الفلسطينية: 1330\n",
            "رئيس: 1286\n",
            "\n",
            "Number of Each Part-of-Speech Tag:\n",
            "JJ: 953\n",
            "NNP: 429723\n",
            "NN: 953\n"
          ]
        }
      ]
    }
  ]
}